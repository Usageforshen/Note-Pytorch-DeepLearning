{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8d86c190dfcadcdaa67edec4a1ea82702241987b5b1f320c920d3d4ca36fee5b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "原文地址如下\n",
    "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "- 原教程中代码里的注释保留英语的同时给出译文，而代码外的教程部分只保留译文。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 通过例子学习PYTORCH\n",
    "- 本教程通过独立的示例介绍了PyTorch的基本概念 。\n",
    "\n",
    "- PyTorch的核心是提供两个主要功能：\n",
    "\n",
    "n维张量，类似于numpy，但可以在GPU上运行\n",
    "自动区分以构建和训练神经网络\n",
    "我们将使用拟合问题 y=sin(x)y=sin⁡(x)以三阶多项式作为我们的运行示例。该网络将具有四个参数，并且将通过使网络输出与真实输出之间的欧几里德距离最小化来进行梯度下降训练，以适应随机数据。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 第一部分 --  张量\n",
    "## 第一部分-张量-example 1-2 numpy和张量\n",
    "## 热身：numpy\n",
    "- 在介绍PyTorch之前，我们将首先使用numpy实现网络。\n",
    "\n",
    "Numpy提供了一个n维数组对象，以及许多用于操纵这些数组的函数。Numpy是用于科学计算的通用框架；它对计算图，深度学习或梯度一无所知。但是，我们可以通过使用numpy操作手动实现通过网络的前向和后向传递，来轻松地使用numpy将三阶多项式拟合为正弦函数："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "# 这个plt是我额外加上去的 只是用作看一下图像\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create random input and output data\n",
    "# 随机生成输入值 再用 sin函数生成对应输出  最后我们自己搞一个三阶多项式来拟合这个sin函数曲线\n",
    "# lispace是在-pi到+pi之间生成2000个数作为输入\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 绘制出x,y构成的图像\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Randomly initialize weights\n",
    "# 随机生成多项式的参数\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "print(\"随机生成的参数---->\",a,b,c,d)\n",
    "\n",
    "# 定义学习率为1e-6\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # 用随机生成参数的多项式来算出对于的预测值y\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    # 计算损失函数 y预测值-y真实值的差求平方 \n",
    "    # loss函数是MSE均方误差\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 反向传播来调整参数abcd\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    # 这里是自己手动求导的过程 因为均方误差是(y_pred-y)的平方 求导后就是 2*(y_pred-y)\n",
    "    grad_y_pred = 2.0 * (y_pred - y)  \n",
    "    # 再用求得的梯度来计算每个参数的梯度 再来更新参数权值\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    # 通过学习率和各个的梯度来更新多项式的权值\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "source": [
    "## PyTorch：张量\n",
    "Numpy是一个很好的框架，但是它不能利用GPU来加速其数值计算。对于现代深度神经网络，GPU通常可以提供50倍或更高的加速比，因此不幸的是，仅凭numpy不足以实现现代深度学习。\n",
    "\n",
    "在这里，我们介绍最基本的PyTorch概念：Tensor。PyTorch张量在概念上与numpy数组相同：张量是n维数组，而PyTorch提供了许多在这些张量上进行操作的功能。在幕后，张量可以跟踪计算图和渐变，但它们也可用作科学计算的通用工具。\n",
    "\n",
    "与numpy不同，PyTorch张量可以利用GPU加速其数字计算。要在GPU上运行PyTorch Tensor，您只需要指定正确的设备即可。\n",
    "\n",
    "- 在这里，我们使用PyTorch张量将三阶多项式拟合为正弦函数。像上面的numpy示例一样，我们需要手动实现通过网络的正向和反向传递："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# 设置设备为CPU 在CPU上运行 如果要在显卡上运行就改字符串为GPU\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "# 随机生成输入值 再用 sin函数生成对应输出  最后我们自己搞一个三阶多项式来拟合这个sin函数曲线\n",
    "# lispace是在-pi到+pi之间生成2000个数作为输入\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "# 随机生成张量作为多项式的参数 同时设置设备为CPU 以及数据类型为浮点型\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "# 定义学习率\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # 计算随机参数下的 预测值y\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "\n",
    "    # 定义损失函数 和上面numpy的例子一样，不过不能直接用numpy的方法 loss仍然是MSE\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "\n",
    "    # 反向传播来调整参数abcd\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    # 这里是自己手动求导的过程 因为均方误差是(y_pred-y)的平方 求导后就是 2*(y_pred-y)\n",
    "    # 得到梯度值之后再对每一个参数求梯度 用作更新参数。\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 再用求得的梯度来计算每个参数的梯度 再来更新参数权值\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}