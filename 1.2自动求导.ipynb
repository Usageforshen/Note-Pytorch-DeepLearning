{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8d86c190dfcadcdaa67edec4a1ea82702241987b5b1f320c920d3d4ca36fee5b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 第一部分 --  张量\n",
    "原文地址如下\n",
    "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "## 第一部分-张量-example 3-4 自动求导\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 自动求导\n",
    "PyTorch：张量和自动求导\n",
    "在以上示例中，我们必须手动实现神经网络的前向和后向传递。对于小型的两层网络，手动实施反向传递并不是什么大问题，但是对于大型的复杂网络而言，可以很快变得非常繁琐。\n",
    "\n",
    "幸运的是，我们可以使用自动微分 来自动计算神经网络中的反向传递。PyTorch中的 autograd软件包完全提供了此功能。使用autograd时，网络的前向传递将定义一个 计算图；图中的节点为张量，边为从输入张量产生输出张量的函数。然后通过该图进行反向传播，可以轻松计算梯度。\n",
    "\n",
    "这听起来很复杂，在实践中使用起来非常简单。每个张量代表计算图中的一个节点。Ifx是一个Tensor， x.requires_grad=True然后x.grad是另一个Tensor，它持有x相对于某个标量值的梯度。\n",
    "\n",
    "在这里，我们使用PyTorch张量和autograd来实现我们的正弦波，并带有三阶多项式示例；现在我们不再需要手动通过网络实现反向传递："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# 设置设备为CPU 在CPU上运行 如果要在显卡上运行就改字符串为GPU\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "# 默认情况下生成张量作为输入和输出requires_grad=False，这代表我们不需要在反向传递过程中针对这些张量计算梯度。\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "# 为权重创建随机的张量。对于三阶多项式，我们需要4个权重：y = a + b x + c x ^ 2 + d x ^ 3 \n",
    "# 设置require_grad = True表示我们要使用在反向传递过程中维护这些张量。\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "#学习率\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    # 前向传递：使用张量上的运算来计算预测的y。\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    # 使用张量上的运算来计算和打印损失。 \n",
    "    # 现在损失是形状的张量（1，）loss.it（）获取损失中保存的标量值。\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    # 使用自动求导计算向后传递。\n",
    "    # 此调用将计算与require_grad = True的所有张量有关的损耗梯度。\n",
    "    # 在此之后，调用a.grad，b.grad。 c.grad和d.grad将由张量保存分别相对于a，b，c，d的损耗梯度。\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # 使用梯度下降手动更新权重。包装在torch.no_grad（）中 \n",
    "    # 因为权重具有require_grad = True，但是我们不需要在自动求导中跟踪它。\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        # 更新权重后手动将梯度归零\n",
    "        # pytorch不同于tf 我们需要手动的设置梯度归零\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "source": [
    "## PyTorch：定义新的自动梯度函数\n",
    "每个原始的自动求导运算符实际上都是在Tensor上运行的两个函数。的正向函数从输入张量计算输出张量。该向后功能接收输出张量的梯度相对于一些标量值，并计算输入张量的梯度相对于该相同标量值。\n",
    "\n",
    "在PyTorch中，我们可以通过定义torch.autograd.Function和实现forward 和backward函数的子类来轻松定义自己的自动求导运算。然后，我们可以通过构造实例并像调用函数一样调用新的自动求导运算，并传递包含输入数据的张量。\n",
    "\n",
    "在此示例中，我们将模型定义为y=a+bP3(c+dx) 代替 y=a+bx+cx2+dx3， 在哪里P3(x)=12(5x3−3x) 是三阶勒让德多项式。我们编写了自己的自定义autograd函数，用于向前和向后计算P3，并使用它来实现我们的模型："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    我们可以通过继承torch.autograd.Function来实现我们自定义的求导函数，并且通过在张量上操作来实现前向传播和反向传播\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        在正向传播过程中我们接受一个包含输入的张量并且返回一个包含输出的张量\n",
    "        ctx是可以使用的上下文对象存放信息以进行向后计算。\n",
    "        您可以通过使用ctx.save_for_backward方法在向后传递缓存任意对象 。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        在后向传递中，我们收到包含损失梯度的张量 关于输出，我们需要计算基于输入的损失梯度。\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "# 设置在CPU上计算\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "# 创建输入和输出张量 \n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "# 为权重创建随机的张量\n",
    "# 还是这个例子，我们需要4个权重：y = a + b * P3（c + d * x），这些权重需要初始化距离正确结果不要太远，以确保收敛。\n",
    "# 设置require_grad = True表示在反向传播的过程中为张量计算梯度\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    # 我们使用Function.apply方法。我们将其别名为“ P3”\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    # 前向通过：使用运算计算预测的y；\n",
    "    # 我们通过使用我们的自定义自动求导操作计算 P3\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    # 自动求导来反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    # 更新权值\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        # 手动梯度清零\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  }
 ]
}